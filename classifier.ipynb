{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# maths and sci libraries.\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# for dict and all.\n",
    "import collections\n",
    "\n",
    "# for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# signal processing\n",
    "from scipy.io                     import wavfile\n",
    "from scipy                        import stats, signal\n",
    "from scipy.fftpack                import fft\n",
    "\n",
    "from scipy.signal                 import lfilter, hamming\n",
    "from scipy.fftpack.realtransforms import dct\n",
    "from scikits.talkbox              import segment_axis\n",
    "from scikits.talkbox.features     import mfcc\n",
    "\n",
    "\n",
    "# encoding purpose.\n",
    "from base64 import b64decode\n",
    "\n",
    "#pandas for csvs.\n",
    "import pandas as pd\n",
    "\n",
    "# import stft\n",
    "import stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the audio files path.\n",
    "audio_file = collections.defaultdict(dict)\n",
    "\n",
    "# initialise the audio songs as (genre, path).\n",
    "# where path is path of the current .wav audio.\n",
    "\n",
    "audio_file[\"rock\"][\"path\"] = r\"tomydeepestego.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audio_length():\n",
    "    samplerate, wavedata = wavfile.read(audio_file[\"rock\"][\"path\"])\n",
    "    audio_file[\"rock\"][\"wavedata\"] = wavedata\n",
    "    audio_file[\"rock\"][\"samplerate\"] = samplerate\n",
    "    number_of_samples = wavedata.shape[0]\n",
    "    print samplerate, wavedata.shape[0]\n",
    "    # song length : number of samples/samplerate.\n",
    "    print \"Audio length: \" + str(number_of_samples/samplerate) + \" seconds\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44100 16181864\n",
      "Audio length: 366 seconds\n"
     ]
    }
   ],
   "source": [
    "audio_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wavedata_mean():\n",
    "    audio_file[\"rock\"][\"wavedata\"] = np.mean(audio_file[\"rock\"][\"wavedata\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wavedata_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Zero Crossing Rate : Is a time domain feature.\n",
    "    Simple, straightforward and inexpensive feature to examine the similarity between two sets of time series.\n",
    "    It is the number of times signal changes sign. It is useful for signals affected by noise.\n",
    "\"\"\"\n",
    "def zero_crossing_rate_bruteForce(wavedata):\n",
    "    zero_crossing = 0\n",
    "    for i in range(1, number_of_samples):\n",
    "        if (wavedata[i-1] < 0 and wavedata[i]>0) or (wavedata[i-1] > 0 and wavedata[i] < 0) or (wavedata[i-1] != 0 and wavedata[i] == 0):\n",
    "            zero_crossing += 1;\n",
    "    zero_crossing_rate = zero_crossing / float(number_of_samples-1)\n",
    "    return zero_crossing_rate\n",
    "\n",
    "\n",
    "def zero_crossing_rate(wavedata, block_length, sample_rate):\n",
    "    # Number of blocks required.\n",
    "    num_blocks = int(np.ceil(len(wavedata)/block_length))\n",
    "    \n",
    "    # Timestamps for the beginning of the blocks.\n",
    "    timestamps = (np.arange(0, num_blocks - 1) * (block_length/float(sample_rate)))\n",
    "    \n",
    "    zcr = []\n",
    "    for i in range(0, num_blocks - 1):\n",
    "        start = i*block_length\n",
    "        stop = np.min([(start + block_length - 1), len(wavedata)])\n",
    "        zc = 0.5*np.mean(np.abs(np.diff(np.sign(wavedata[start:stop]))))\n",
    "        zcr.append(zc)\n",
    "        \n",
    "    return np.asarray(zcr), np.asarray(timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zcr, zcr_timestamps = zero_crossing_rate(audio_file[\"rock\"][\"wavedata\"], 1024, audio_file[\"rock\"][\"samplerate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ...,  0.02446184,\n",
       "        0.04354207,  0.03620352])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0400f6410>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(zcr_timestamps, zcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Root mean Square : comparing arbitary waveforms based upon \n",
    "    their equivalent energy.\"\"\"\n",
    "def root_mean_square(wavedata, block_length, sample_rate):\n",
    "    num_blocks = int(np.ceil(len(wavedata)/block_length))\n",
    "    \n",
    "    timestamps = (np.arange(0, num_blocks-1) * (block_length/float(sample_rate)))\n",
    "    \n",
    "    rms = []\n",
    "    \n",
    "    for i in range(0, num_blocks-1):\n",
    "        start = i*block_length\n",
    "        stop = np.min([(start + block_length -1), len(wavedata)])\n",
    "        \n",
    "        rms_seg = np.sqrt(np.mean(wavedata[start:stop]**2))\n",
    "        rms.append(rms_seg)\n",
    "    return np.asarray(rms), np.asarray(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms, rms_timestamps = root_mean_square(audio_file[\"rock\"][\"wavedata\"], 1024, audio_file[\"rock\"][\"samplerate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0091c3150>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(rms_timestamps, rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Spectral features. \"\"\"\n",
    "\"\"\" Spectral centroid : centre of gravity.\n",
    "    Tells the frequency around which most of the signal energy is concentrated.\n",
    "    Tells how dark/bright the sound is.\"\"\"\n",
    "def spectral_centroid(wavedata, window_size, sample_rate):\n",
    "    magnitude_spectrum = stft.spectrogram(wavedata, window_size)\n",
    "    timebins, freqbins = np.shape(magnitude_spectrum)\n",
    "    timestamps = (np.arange(0, timebins - 1)*(timebins/float(sample_rate)))\n",
    "    spec_centroid = []\n",
    "    \n",
    "    for t in range(timebins - 1):\n",
    "        power_spectrum = np.abs(magnitude_spectrum[t])**2\n",
    "        \n",
    "        sc_t = np.sum(power_spectrum * np.arange(1, freqbins + 1))/np.sum(power_spectrum)\n",
    "        \n",
    "        spec_centroid.append(sc_t)\n",
    "        \n",
    "    return np.nan_to_num(np.asarray(spec_centroid)), np.asarray(timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_centroid, spec_ts = spectral_centroid(audio_file[\"rock\"][\"wavedata\"], 1024, audio_file[\"rock\"][\"samplerate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0091ed750>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(spec_ts, spec_centroid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Sprectral rolloff :\n",
    "    Nth percentile of the power spectral distribution, \n",
    "    where N is 85% or 95%. The rolloff point is the frequency below which \n",
    "    the N% of the magnitude distribution is concentrated.\n",
    "    Used to distinguish voice speech from unvoiced.\n",
    "    Unvoiced has a high proportion of energy contained in the high-frequency range of the spectrum.\n",
    "    - fraction of bins in the power spectrum at which 85%(N%) of the power is at lower frequencies.\n",
    "\"\"\"\n",
    "def spectral_rolloff(wavedata, window_size, sample_rate, k=0.85):\n",
    "    # convert into frequency domain using short term fourier transform.\n",
    "    magnitude_spectrum = stft.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Sprectral rolloff :\n",
    "    Nth percentile of the power spectral distribution, \n",
    "    where N is 85% or 95%. The rolloff point is the frequency below which \n",
    "    the N% of the magnitude distribution is concentrated.\n",
    "    Used to distinguish voice speech from unvoiced.\n",
    "    Unvoiced has a high proportion of energy contained in the high-frequency range of the spectrum.\n",
    "    - fraction of bins in the power spectrum at which 85%(N%) of the power is at lower frequencies.\n",
    "\"\"\"\n",
    "def spectral_rolloff(wavedata, window_size, sample_rate, k=0.85):\n",
    "    # convert into frequency domain using short term fourier transform.\n",
    "    magnitude_spectrum = stft.spectrogram(wavedata, window_size)\n",
    "    time_bins, freq_bins = np.shape(magnitude_spectrum)\n",
    "    power_spectrum = np.abs(magnitude_spectrum)**2\n",
    "    \n",
    "    # create timestamps\n",
    "    timestamps = (np.arange(0, time_bins - 1)*(time_bins/float(sample_rate)))\n",
    "        \n",
    "    spec_rolloff = []\n",
    "    \n",
    "    spectral_sum = np.sum(power_spectrum, axis=1)\n",
    "    \n",
    "    for t in range(time_bins - 1):\n",
    "        # find frequency-bin indices where cummulative sum of all bins is higher than k-percent of the sum of all bins.\n",
    "        # minimum index = rolloff.\n",
    "        spec_rolloff_temp = np.where(np.cumsum(power_spectrum[t, :]) >= k*spectral_sum[t])[0][0]\n",
    "        spec_rolloff.append(spec_rolloff_temp)\n",
    "    \n",
    "    spec_rolloff = np.asarray(spec_rolloff).astype(float)\n",
    "    \n",
    "    spec_rolloff = (spec_rolloff/freq_bins)*(sample_rate/2.0)\n",
    "    \n",
    "    return spec_rolloff, np.asarray(timestamps)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_rolloff, spec_ts = spectral_rolloff(audio_file[\"rock\"][\"wavedata\"], 1024, audio_file[\"rock\"][\"samplerate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0091b1e90>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(spec_ts, spec_rolloff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Spectral flux : squared diff in frequency distribution of two successive time frames.\"\"\"\n",
    "\"\"\" Helps in measuring rate of local change in the spectrum\"\"\"\n",
    "def spectral_flux(wavedata, window_size, sample_rate):\n",
    "    magnitude_spectrum = stft.spectrogram(wavedata, window_size)\n",
    "    time_bins, freq_bins = np.shape(magnitude_spectrum)\n",
    "    \n",
    "    # create timestamps.\n",
    "    timestamps = (np.arange(0, time_bins - 1) * (time_bins/float(sample_rate)))\n",
    "    \n",
    "    spec_flux = np.sqrt(sp.sum(np.diff(np.abs(magnitude_spectrum))**2, axis = 1))/freq_bins\n",
    "    \n",
    "    return spec_flux[1:], np.asarray(timestamps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec_flux, spec_flux_ts = spectral_flux(audio_file[\"rock\"][\"wavedata\"], 1024, audio_file[\"rock\"][\"samplerate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0091d7f90>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(spec_flux_ts, spec_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" MFCC : coefficients that collectively make up an MFC.\n",
    "    They are derived from a type of cepstral representation of the audio ( a nonlinear spectrum of a spectrum).\n",
    "    In MFC the frequency bands are equally spaced on the mel scale, which approximates the human auditory systems response more closely than the linear spaced frequency bands.\n",
    "\"\"\"\n",
    "def MFCC_Cal(input_data):\n",
    "    # apply pre-filtering.\n",
    "    \n",
    "    # params\n",
    "    nwin = 256\n",
    "    nfft = 1024\n",
    "    fs = 16000\n",
    "    nceps = 13\n",
    "    \n",
    "    # pre-emphasis factor\n",
    "    prefac = 0.97\n",
    "    \n",
    "    over = nwin - 160\n",
    "    \n",
    "    filtered_data = lfilter([1., -prefac], 1, input_data)\n",
    "    \n",
    "    # compute the spectrum amplitude by windowing with a hamming window.\n",
    "    windows = hamming(256, sym = 0)\n",
    "    framed_data = segment_axis(filtered_data, nwin, over) * windows\n",
    "    \n",
    "    magnitude_spectrum = np.abs(fft(framed_data, nfft, axis = -1))\n",
    "    \n",
    "    \n",
    "    # filter the signal in the spectral domain with a triangular filter-bank, \n",
    "    # whose filters are approximately linearly spaced on the mel scale, and have equal bandwidth in the mel scale/\n",
    "\n",
    "    lowfreq = 133.33\n",
    "    linsc = 200/3\n",
    "    logsc = 1.0711703\n",
    "    fs = 44100\n",
    "    \n",
    "    nlinfilt = 13\n",
    "    nlogfilt = 27\n",
    "    \n",
    "    #total filters \n",
    "    nfilt = nlinfilt + nlogfilt\n",
    "    \n",
    "    # Compute the filter bank.\n",
    "    # compute start/middle/end points of the triangular filters in spectral.\n",
    "    \n",
    "    #domain.\n",
    "    freqs = np.zeros(nfilt + 2)\n",
    "    freqs[:nlinfilt] = lowfreq + np.arange(nlinfilt) * linsc\n",
    "    \n",
    "    freqs[nlinfilt:] = freqs[nlinfilt - 1] * logsc ** np.arange(1, nlogfilt + 3)\n",
    "    \n",
    "    heights = 2./(freqs[2:] - freqs[0:-2])\n",
    "    \n",
    "    #compute filterbank coeff (in fft domain, in bins)\n",
    "    filterbank = np.zeros((nfilt, nfft))\n",
    "    \n",
    "    # FFT bins (in Hz)\n",
    "    nfreqs = np.arange(nfft)/(1. * nfft)*fs\n",
    "    \n",
    "    for i in range(nfilt):\n",
    "        low = freqs[i]\n",
    "        cen = freqs[i+1]\n",
    "        hi = freqs[i+2]\n",
    "        \n",
    "        lid = np.arange(np.floor(low*nfft/fs) + 1,\n",
    "                        np.floor(cen*nfft/fs) + 1, dtype = np.int)\n",
    "        \n",
    "        rid = np.arange(np.floor(cen*nfft/fs) + 1,\n",
    "                        np.floor(hi*nfft/fs) + 1, dtype = np.int)\n",
    "        \n",
    "        lslope = heights[i]/(cen - low)\n",
    "        rslope = heights[i]/(hi-cen)\n",
    "        \n",
    "        filterbank[i][lid] = lslope * (nfreqs[lid] - low)\n",
    "        filterbank[i][rid] = rslope * (hi - nfreqs[rid])\n",
    "        \n",
    "        # filter the spectrum through the triangle filterbank.\n",
    "        \n",
    "        mspec = np.log10(np.dot(magnitude_spectrum, filterbank.T))\n",
    "        \n",
    "        \n",
    "        # Use the DCT to 'compress' the coefficients (spectrum -> cepstrum domain)\n",
    "        MFCCs = dct(mspec, type = 2, norm = 'ortho', axis = -1)[:, :nceps]\n",
    "        \n",
    "        \n",
    "        return MFCCs, mspec, magnitude_spectrum\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MFCCs, mspec, spec = MFCC_Cal(audio_file[\"rock\"][\"wavedata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Rhythm patterns : \n",
    "    Describe modulation amplitudes for a range of modulation frequencies on \"critical bands\" of the human auditory range.\n",
    "    Two step process. \n",
    "    1) The specific loudness sensation in different frequency bands is computed, grouping the resulting frequency bands to psycho-acoustically motivated critical-bands. \n",
    "        -> This results in human loudness sensation. (Sonogram)\n",
    "    2) The spectrum is transformed into a time-invariant representation based on modulation frequency, which is achieved by applying DFT. resulting in amplitude modulations of the loudness in individual critical bands.\n",
    "    Amplitude modulations having different effects on human hearing sensation depending on their frequency, the most significant of which, referred to as fluctuation strength, is most intense is 4Hz and decreasing towards 15Hz.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def spectrograph(wave_data, sample_rate):\n",
    "    # parameters.\n",
    "    skip_leading_fadeout = 1\n",
    "    step_width = 3\n",
    "    \n",
    "    segment_size = 2**18\n",
    "    fft_window_size = 1024 # for 44100 Hz\n",
    "    \n",
    "    \n",
    "    # required precalculations.\n",
    "    \n",
    "    duration = data.shape[0]/sample_rate\n",
    "    # calculate frequency values on y-axis (for bark scale calculation)\n",
    "    freq_axis = float(fs)/fft_window_size * np.arange(0, (fft_window_size/2) + 1)\n",
    "    mod_freq_res = 1/(float(segment_size)/fs)\n",
    "    mod_freq_axis = mod_freq_res * np.arange(257) # modulation frequencies along.\n",
    "    # x-axis from index 1 to 257\n",
    "    fluct_curve = 1/(mod_freq_axis/4 + 4/mod_freq_axis)\n",
    "    \n",
    "    skip_seg = skip_leading_fadeout\n",
    "    seg_pos = np.array([1, segment_size])\n",
    "    \n",
    "    if ((skip_leading_fadeout > 0) or (step_width > 1)):\n",
    "        if duration < 45 :\n",
    "            step_width = 1\n",
    "            skip_seg = 0\n",
    "        else:\n",
    "            seg_pos = seg_pos + segment_size * skip_seg\n",
    "    \n",
    "    wavsegment = data[seg_pos[0] - 1: seg_pos[1]]\n",
    "    \n",
    "    wavsegment = 0.0875 * wavsegment * (2**15)\n",
    "    \n",
    "    n_iter = wavsegment.shape[0]/fft_window_size*2 - 1\n",
    "    w = np.hanning(fft_window_size)\n",
    "    \n",
    "    spectrograph = np.zeros((fft_window_size/2 + 1), n_iter)\n",
    "    \n",
    "    idx = np.arange(fft_window_size)\n",
    "    \n",
    "    # stepping through the wave segment, building spectrum for each window.\n",
    "    for i in range(n_iter):\n",
    "        spectrogr[:,i] = periodogram(x = wavsegment[idx], win = w)\n",
    "        idx = idx + fft_window_size/2\n",
    "    Pxx = spectrogr\n",
    "    return Pxx\n",
    "\n",
    "def bark_scale(Pxx):\n",
    "    # calculate bark-filterbank\n",
    "    loudn_bark = np.zeros((eq_loudness.shape[0], len(bark)))\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    for bsi in bark:\n",
    "        \n",
    "        while j < len(loudn_freq) and bsi > loudn_freq[i]:\n",
    "            j+=1\n",
    "        j-=1\n",
    "        \n",
    "        if np.where(loudn_freq == bsi)[0].size != 0:\n",
    "            loudn_bark[:, i] = eq_loudness[:, np.where(loudn_freq == bsi)][:,0, 0]\n",
    "        else:\n",
    "            w1 = 1/np.abs(loudn_freq[j] - bsi)\n",
    "            w2 = 1/np.abs(loudn_freq[j+1] - bsi)\n",
    "            loudn_bark[:, i] = (eq_loudness[:, j]*w1 + eq_loudness[:, j+1]*w2)/(w1+w2)\n",
    "        i+=1\n",
    "        \n",
    "    # Apply bark-filter \n",
    "    matrix = np.zeros((len(bark), Pxx.shape[1]))\n",
    "    \n",
    "    barks = bark[:]\n",
    "    barks.insert(0, 0)\n",
    "    \n",
    "    for i in range(len(barks) - 1):\n",
    "        martix[i] = np.sum(Pxx[((freq_axis >= barks[i]) & (freq_axis < barks[i+1]))], axis = 0)\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bark = [100,   200,  300,  400,  510,  630,   770,   920, \n",
    "        1080, 1270, 1480, 1720, 2000, 2320,  2700,  3150,\n",
    "        3700, 4400, 5300, 6400, 7700, 9500, 12000, 15500]\n",
    "\n",
    "eq_loudness = np.array(\n",
    "    [[ 55,   40,  32,  24,  19,  14, 10,  6,  4,  3,  2,  \n",
    "        2,    0,  -2,  -5,  -4,   0,  5, 10, 14, 25, 35], \n",
    "     [ 66,   52,  43,  37,  32,  27, 23, 21, 20, 20, 20,  \n",
    "       20,   19,  16,  13,  13,  18, 22, 25, 30, 40, 50], \n",
    "     [ 76,   64,  57,  51,  47,  43, 41, 41, 40, 40, 40,\n",
    "     39.5, 38,  35,  33,  33,  35, 41, 46, 50, 60, 70], \n",
    "     [ 89,   79,  74,  70,  66,  63, 61, 60, 60, 60, 60,  \n",
    "       59,   56,  53,  52,  53,  56, 61, 65, 70, 80, 90], \n",
    "     [103,   96,  92,  88,  85,  83, 81, 80, 80, 80, 80,  \n",
    "       79,   76,  72,  70,  70,  75, 79, 83, 87, 95,105], \n",
    "     [118,  110, 107, 105, 103, 102,101,100,100,100,100,  \n",
    "       99,   97,  94,  90,  90,  95,100,103,105,108,115]])\n",
    "\n",
    "loudn_freq = np.array(\n",
    "    [31.62,   50,  70.7,   100, 141.4,   200, 316.2,  500, \n",
    "     707.1, 1000,  1414,  1682,  2000,  2515,  3162, 3976,\n",
    "     5000,  7071, 10000, 11890, 14140, 15500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Spectral Masking : \n",
    "    occlusion of a quiet sound by a louder soung when both sounds are present \n",
    "    simultaneously and have similar frequencies.\n",
    "    Masking can be categorized as : \n",
    "    1) simultaneous masking : two sounds active simultaneously.\n",
    "    2) post-masking : a sound closely following it (100-200ms).\n",
    "    3) pre-masking : a sound preceding it.\n",
    "    \n",
    "\"\"\"\n",
    "def spectral_masking(matrix):\n",
    "    n_bark_bands = len(bark)\n",
    "    \n",
    "    Const_spread = np.zeros((n_bark_bands, n_bark_bands))\n",
    "    \n",
    "    for i in range(n_bark_bands):\n",
    "        Const_spread[i, :] = 10**((15.81 + 7.5*((i-np.arange(n_bark_bands)) + 0.474) - 17.5*(1 + ((i - np.arange(n_bark_bands)) + 0.474)**2)**0.5)/10)\n",
    "    spread = Const_spread[:matrix.shape[0], :]\n",
    "    matrix = np.dot(spread, matrix)\n",
    "    \n",
    "    # map to decibel scale\n",
    "    matrix[np.where(matrix < 1)] = 1\n",
    "    matrix = 10*np.log10(matrix)\n",
    "\n",
    "\"\"\" Phon Scale : Equal loudness curves (Phon)\n",
    "    \n",
    "    relationship between sound pressure level in decibel and perceived hearing sensation.\n",
    "    each loudness contours for 3,20, 40, 60, 80, 100 phon\"\"\"\n",
    "\n",
    "def phon_mapping(matrix):\n",
    "    n_bands = matrix.shape[0]\n",
    "    t = matrix.shape[1]\n",
    "    \n",
    "    # DB to Phon bark-scale-limit table!\n",
    "    \n",
    "    # introducing 1 level more with level(1) being infinite to avoid (levels-1) producing errors like division by 0.\n",
    "    \n",
    "    table_dim = n_bands\n",
    "    cbv = np.concatenate((np.tile(np.inf, (table_dim,1)), loudn_freq[:, 0:n_bands].transpose()), 1)\n",
    "    \n",
    "    phons = phon[:]\n",
    "    phons.insert(0,0)\n",
    "    phons = np.asarray(phons)\n",
    "    \n",
    "    #init lowest level = 2\n",
    "    levels = np.tile(2, (n_bands, t))\n",
    "    \n",
    "    for lev in range(1, 6):\n",
    "        db_thislev = np.tile(np.asarray([cbv[:, lev]]).transpose(), (1,t))\n",
    "        levels[np.where(matrix > cb_thislev)] = lev+2\n",
    "    # the matrix 'levels' stores the correct Phon level for each datapoint\n",
    "    cbv_ind_hi = np.ravel_multi_index(dims = (table_dim,7), multi_index = np.array([np.tile(np.array([range(0, table_dim)]).transpose(), (1, t)), levels-1]), order = 'F')\n",
    "    \n",
    "    cbv_ind_lo = np.ravel_multi_index(dims = (table_dim, 7), multi_index = np.array([np.tile(np.array([range(0, table_dim)]).transpose(), (1, t), levels-2)]), order = 'F')\n",
    "    \n",
    "    # interpolation factor % OPT : pre-calc diff\n",
    "    ifac = (matrix[:, 0:t] - cbv.transpose().ravel()[cbv_ind_lo])/(cbv.transpose().ravel()[cbv_ind_hi] - cbv.transpose().ravel()[cbv_ind_lo])\n",
    "    \n",
    "    # keeps the upper phon value\n",
    "    ifac[np.where(levels==2)] = 1\n",
    "    ifac[np.where(levels == 8)] = 1\n",
    "    \n",
    "    matrix[: , 0:t] = phons.transpose().ravel()[levels-2] + (ifac * (phons.transpose().ravel()[levels-1] - phons.transpose().ravel()[levels-2]))\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Transform to Sone scale.\n",
    "\n",
    "# Sone : 1, 2, 4, 8, 16, 32, 64. Phon : 40, 50, 60, 70, 80, 90, 100\n",
    "\n",
    "def sone_scale(matrix):\n",
    "    idx = np.where(matrix >= 40)\n",
    "    not_idx = np.where(matrix < 40)\n",
    "    \n",
    "    matrix[idx] = 2**((matrix[idx] - 40)/10)\n",
    "    matrix[not_idx] = (matrix[not_idx]/40)**2.642\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Statistical Spectrum Descriptors : Rhythimic content of a piece of audio by computing the statistical moments on the sonogram values of each of the critical bands : mean, median, variance, skewness, kurtosis, min and max values.\n",
    "\"\"\"\n",
    "def calc_statistical_features(matrix):\n",
    "    result = np.zeros((matrix.shape[0], 7))\n",
    "    result[:, 0] = np.mean(matrix, axis = 1)\n",
    "    result[: ,1] = np.var(matrix, axis=1)\n",
    "    result[:, 2] = scipy.stats.skew(matrix, axis = 1)\n",
    "    result[:, 3] = scipy.stats.kurtosis(matrix, axis = 1)\n",
    "    result[:, 4] = np.median(matrix, axis = 1)\n",
    "    result[:, 5] = np.min(matrix, axis = 1)\n",
    "    result[:, 6] = np.max(matrix, axis = 1)\n",
    "    \n",
    "    result = np.nan_to_num(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
